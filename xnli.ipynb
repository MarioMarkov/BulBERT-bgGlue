{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Users\\mario\\.conda\\envs\\parking\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoTokenizer,\\\n",
    "AutoModelForTokenClassification, AutoModelForMaskedLM, TrainingArguments, Trainer,AutoModelForSequenceClassification,DataCollatorWithPadding\n",
    "import evaluate\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset,Dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 5010\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 2490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Load Dataset\n",
    "hf_dataset = load_dataset(\"bgglue/bgglue\",\"xnlibg\")\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['entailment', 'neutral', 'contradiction'], 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = hf_dataset[\"train\"].features[\"label\"].names\n",
    "label_list, len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mor40/BulBERT-chitanka-model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_checkpoint = \"mor40/BulBERT-chitanka-model\"\n",
    "model_raw = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'концептуално крем краде има две основни измерения - продукт и география .',\n",
       " 'hypothesis': 'продукт и география са това , което прави крем краде работа .',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 - entailment\n",
    "1 - neutral\n",
    "2 - contradiction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on validation dataset: 100%|██████████| 2490/2490 [00:00<00:00, 3102.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "        # Tokenize the texts\n",
    "        return tokenizer(\n",
    "            examples[\"premise\"],\n",
    "            examples[\"hypothesis\"],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "eval_dataset = hf_dataset[\"validation\"].map(\n",
    "                preprocess_function,\n",
    "                batched=True,\n",
    "                desc=\"Running tokenizer on validation dataset\",\n",
    "                remove_columns = [\"premise\", \"hypothesis\"]\n",
    ")\n",
    "test_dataset = hf_dataset[\"test\"].map(\n",
    "                preprocess_function,\n",
    "                batched=True,\n",
    "                desc=\"Running tokenizer on test dataset\",\n",
    "                remove_columns = [\"premise\", \"hypothesis\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"mor40/BulBERT-xnli-2epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\Users\\mario\\Desktop\\MLPython\\bgGlue\\xnli.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Users/mario/Desktop/MLPython/bgGlue/xnli.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Users/mario/Desktop/MLPython/bgGlue/xnli.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     logits \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mexample)\u001b[39m.\u001b[39mlogits[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Users/mario/Desktop/MLPython/bgGlue/xnli.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m logits\n",
      "File \u001b[1;32mf:\\Users\\mario\\.conda\\envs\\parking\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\Users\\mario\\.conda\\envs\\parking\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1564\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1565\u001b[0m     input_ids,\n\u001b[0;32m   1566\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1567\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1568\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1569\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1570\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1571\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1572\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1573\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1574\u001b[0m )\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1578\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mf:\\Users\\mario\\.conda\\envs\\parking\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\Users\\mario\\.conda\\envs\\parking\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:970\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    969\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[1;32m--> 970\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[0;32m    971\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    972\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**example).logits[0]\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_pt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\Users\\mario\\Desktop\\MLPython\\bgGlue\\xnli.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Users/mario/Desktop/MLPython/bgGlue/xnli.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m example_pt\n",
      "\u001b[1;31mNameError\u001b[0m: name 'example_pt' is not defined"
     ]
    }
   ],
   "source": [
    "example_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2490/2490 [05:18<00:00,  7.81 examples/s]\n",
      "Map: 100%|██████████| 5010/5010 [08:56<00:00,  9.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "id2label ={\n",
    "      0 : \"entailment\",\n",
    "      1 :'neutral',\n",
    "      2 : 'contradiction'\n",
    "}\n",
    "def get_predictions(example, model):\n",
    "      inputs = tokenizer( example[\"premise\"],example[\"hypothesis\"],padding=True,truncation=True,return_tensors='pt')\n",
    "      # Run through model\n",
    "      with torch.no_grad():\n",
    "            logits = model(**inputs).logits[0]\n",
    "\n",
    "      label_id = np.argmax(logits).item()\n",
    "      label_as_word = id2label[label_id]\n",
    "      example[\"predicted_label_id\"] = label_id\n",
    "      example[\"predicted_label\"] = label_as_word\n",
    "      return example\n",
    "dataset_validation = hf_dataset[\"validation\"].map(get_predictions,fn_kwargs={\"model\": model})\n",
    "dataset_test= hf_dataset[\"test\"].map(get_predictions,fn_kwargs={\"model\": model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'hypothesis', 'label', 'predicted_label_id', 'predicted_label'],\n",
       "    num_rows: 2490\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7016064257028113\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i, example in enumerate(dataset_validation):\n",
    "  if example[\"predicted_label_id\"] == example[\"label\"]: correct+=1\n",
    "\n",
    "print(\"Accuracy: \" ,correct / len(dataset_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for i, example in enumerate(dataset_test):\n",
    "  rec = {}\n",
    "  rec['index'] = i\n",
    "  rec['label'] = example[\"predicted_label\"]\n",
    "  predictions.append(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "with jsonlines.open('predictions_xnli_2epochs.jsonl', 'w') as writer:\n",
    "    writer.write_all(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
